{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b083dfcb-8505-40a6-88e7-ad1520ac7e7b",
   "metadata": {},
   "source": [
    "# What is Tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5e231-da2e-420f-a7d7-ad44457d6fe8",
   "metadata": {},
   "source": [
    "Tokenizers are one of the most important tools in nlp, which break down text into smaller units called tokens. These tokens can be words, characters or subwords, making complex sentence understandable to computers. Mainly tokenizers bridge the gap between human language and machine understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa8ec5-d407-4cba-8683-660ec48b51e0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0db4d-a394-43ba-a745-35922195fef0",
   "metadata": {},
   "source": [
    "For this lab, the following libraries are gonna be used:\n",
    "\n",
    "* [`nltk`](https://www.nltk.org/) or natural language toolkit, will be employed for `data management` tasks. It offers comprehensive tools and resources for processing natural language task, making it a valuable choice for tasks such as text preprocessing, and analysis\n",
    "\n",
    "* [`spaCy`](https://spacy.io/) is an open-source library for advanced natural preocessing in Python. `spaCy` is renowned for its speed and accuracy in processing large volumes of text data\n",
    "\n",
    "* [`BertTokenizer`](https://huggingface.co/docs/transformers/main_classes/tokenizer#berttokenizer) is part of the Hugging Face Transformers Library, a popular library for working with state-of-the-art pre-trained language models. `BertTokenizer` is specially designed for `tokenizing` text according to the BERT model's specifications.\n",
    "\n",
    "* [`XLNetTokenizer`](https://huggingface.co/docs/transformers/main_classes/tokenizer#xlnettokenizer) is another component of the Hugging Face Transformers library. It is tailored for tokenizing text in alignment with the XLNet model's requirements.\n",
    "\n",
    "* [`torchtext`](https://pytorch.org/text/stable/index.html) It is part of the PyTorch ecosystem, to handle various natural language processing tasks. It  simplifies the process of working with text data and provides functionalities for data preprocessing, tokenization, vocabulary management, and batching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb74eac-bc3b-4a66-b19d-40bd560612d1",
   "metadata": {},
   "source": [
    "# Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6ee975-07aa-4807-aa16-44d3a56100cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.42.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (4.42.1)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.30.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (2025.1.31)\n",
      "Requirement already satisfied: sentencepiece in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: spacy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Obtaining dependency information for numpy>=1.19.0 from https://files.pythonhosted.org/packages/2b/3e/e7247c1d4f15086bb106c8d43c925b0b2ea20270224f5186fa48d4fb5cbd/numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Using cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.2.4 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.2.4 which is incompatible.\n",
      "mediapipe 0.10.15 requires numpy<2, but you have numpy 2.2.4 which is incompatible.\n",
      "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.2.4 which is incompatible.\n",
      "scipy 1.11.1 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/h5py/__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py/_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py/h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py/h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/thinc/compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"/Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages/h5py/__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py/_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py/h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py/h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Requirement already satisfied: scikit-learn in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Obtaining dependency information for numpy>=1.19.5 from https://files.pythonhosted.org/packages/1a/2e/151484f49fd03944c4a3ad9c418ed193cfd02724e138ac8a9505d056c582/numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (2025.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: torchtext==0.17.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2->torchtext==0.17.2) (2025.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
      "Collecting numpy==1.26.0\n",
      "  Obtaining dependency information for numpy==1.26.0 from https://files.pythonhosted.org/packages/35/21/9e150d654da358beb29fe216f339dc17f2b2ac13fff2a89669401a910550/numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (99 kB)\n",
      "Using cached numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.0 which is incompatible.\n",
      "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5a407-33ae-455d-a772-6d3e87ac9658",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3175ee22-ebf7-40da-b4d0-13e90889ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn=warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0e10b-67f7-4693-b240-179b5f324628",
   "metadata": {},
   "source": [
    "## Why tokenization is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99770e8e-8229-476b-813b-9cdc06abe868",
   "metadata": {},
   "source": [
    "Tokenization segmenting text into smaller units called tokens. These tokens are subsequently transformed into `numerical representation` called token indices, which are directly employed by deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe979aef-ea92-4532-a53b-45b42374a8ae",
   "metadata": {},
   "source": [
    "## Types of tokenizer\n",
    "\n",
    "Tokenization methods are further divided into 3 main sections.\n",
    "\n",
    "    * Word-Based\n",
    "    * Character-Based\n",
    "    * Subword-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aecc9-c651-437a-8c31-315d4f1659c6",
   "metadata": {},
   "source": [
    "## Word-based Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76265949-bb38-4304-855a-60819b316026",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08c162-0506-41d7-b9b9-4f8f09f53e55",
   "metadata": {},
   "source": [
    "As the name suggests, this is the splitting of text based on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f96e484-e2c5-4cb5-9fb5-c781784b35a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a sample sentence for word tokenization'\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20892666-a1b2-42c0-b3a1-cf4c68ad2cc8",
   "metadata": {},
   "source": [
    "General libraries like nltk and spaCy often split words like 'don't' and 'couldn't,' which are contractions, into different individual words. There's no universal rule, and each library has its own tokenization rules for word-based tokenizers. However, the general guideline is to preserve the input format after tokenization to match how the model was trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cedd6531-4863-43ec-9bb2-c73c391f0e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'help',\n",
       " 'the',\n",
       " 'dog',\n",
       " '.',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'do',\n",
       " 'it',\n",
       " '?',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This showcases word_tokenize from nltk library\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a02117a-4ad1-4ca3-a36d-6d9e2c434132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday\"\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7c963-1624-485d-a2ef-e74e19025543",
   "metadata": {},
   "source": [
    "The problem with this algorithm is that words with similar meanings will be assigned different IDs, resulting in them being treated as entirely separate words with distinct meanings. For example, $Unicorns$ is the plural form of $Unicorn$, but a word-based tokenizer would tokenize them as two separate words, potentially causing the model to miss their semantic relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a108f53-c817-48bf-ab04-6b353b698d7e",
   "metadata": {},
   "source": [
    "Each word is split into a token, leading to a significant increase in the model's overall vocabulary. Each token is mapped to a large vector containing the word's meanings, resulting in large model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f190a-3c77-4f0b-9bf6-bd40c1a06416",
   "metadata": {},
   "source": [
    "# Character Based Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537f6d6-aeba-4410-a9a3-f14526871187",
   "metadata": {},
   "source": [
    "As the name suggests, character-based tokenization involves splitting text into individual characters. The advantage of using this approach is that the resulting vocabularies are inherently small. Furthermore, since languages have a limited set of characters, the number of out-of-vocabulary tokens is also limited, reducing token wastage.\n",
    "\n",
    "For example:\n",
    "Input text: `This is a sample sentence for tokenization.`\n",
    "\n",
    "Character-based tokenization output: `['T', 'h', 'i', 's', 'i', 's', 'a', 's', 'a', 'm', 'p', 'l', 'e', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 'f', 'o', 'r', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.']`\n",
    "\n",
    "However, it's important to note that character-based tokenization has its limitations. Single characters may not convey the same information as entire words, and the overall token length increases significantly, potentially causing issues with model size and a loss of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277efd0-9190-4f59-bbb4-60b4f08abf29",
   "metadata": {},
   "source": [
    "## Subword Based Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a004d-2b09-4041-ba59-d378c482ea6a",
   "metadata": {},
   "source": [
    "The subword-based tokenizer allows frequently used words to remian unsplit while breakind down infrequent words into meaningful subwords. Techniques such as `WordPiece` and `SentencePiece` are commonly used for subword tokenization.These methods learn subword units from a given text corpus, identifying common prefixes, suffixes, and root words as subword tokens based on their frequency of occurrence. This approach offers the advantage of representing a broader range of words and adapting to the specific language patterns within a text corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36b6d0-000a-488f-9a90-4808d183eae7",
   "metadata": {},
   "source": [
    "1. `Unhappiness`-> 'Un' & 'Happiness'\n",
    "2. `Unicorns` -> 'Unicorn' & 's'\n",
    "\n",
    "In both examples below, words are split into subwords, which helps preserve the semantic information associated with the overall word. For instance, 'Unhappiness' is split into 'un' and 'happiness,' both of which can appear as stand-alone subwords. When we combine these individual subwords, they form 'unhappiness,' which retains its meaningful context. This approach aids in maintaining the overall information and semantic meaning of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080c1b5-5123-4756-ac71-b715f8e271a7",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "\n",
    "Initially, WordPiece initializes its vocabulary to include every `character present` in the training data and progresively learns a `specified number of merge rules`\n",
    "So, it basically does character based tokenization first, and then progressively merges characters with one another to form words.\n",
    "\n",
    "Now, the WordPiece tokenizer is implemented in BertTokenizer. Note that, BertTokenizer treats composite words as separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46894a02-ada8-4c2e-b613-019b7d9a96a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6102a-f2d1-4291-854d-40992f886e70",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the output:\n",
    "- 'ibm': \"IBM\" is tokenized as 'ibm'. BERT converts tokens into lowercase, as it does not retain the case information when using the \"bert-base-uncased\" model.\n",
    "- 'taught', 'me', '.': These tokens are the same as the original words or punctuation, just lowercased (except punctuation).\n",
    "- 'token', '##ization': \"Tokenization\" is broken into two tokens. \"Token\" is a whole word, and \"##ization\" is a part of the original word. The \"##\" indicates that \"ization\" should be connected back to \"token\" when detokenizing (transforming tokens back to words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac1a3a-4b89-4a8d-81d2-3c5e41127158",
   "metadata": {},
   "source": [
    "## Unigram and SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67693e99-5153-42d9-af38-74067d067760",
   "metadata": {},
   "source": [
    "Unigram is a method for breaking words or text into smaller pieces. It accomplishes this by starting with a large list of possibilities and gradually narrowing it down based on how frequently those pieces appear in the text. This approach aids in efficient text tokenization.\n",
    "\n",
    "SentencePiece is a tool that takes text, divides it into smaller, more manageable parts, assigns IDs to these segments, and ensures that it does so consistently. Consequently, if you use SentencePiece on the same text repeatedly, you will consistently obtain the same subwords and IDs.\n",
    "\n",
    "Unigram and SentencePiece work together by implementing Unigram's subword tokenization method within the SentencePiece framework. SentencePiece handles subword segmentation and ID assignment, while Unigram's principles guide the vocabulary reduction process to create a more efficient representation of the text data. This combination is particularly valuable for various NLP tasks in which subword tokenization can enhance the performance of language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd221f88-2781-4471-bfb1-b57cc2683132",
   "metadata": {},
   "source": [
    "`Unigram` helps you figure out which subwords should be part of the model's vocabulary by looking at frequency.\n",
    "\n",
    "`SentencePiece` is the tool that applies Unigram's method to break text into those subwords, ensuring consistency and assigning IDs to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a56489af-fb1d-47b5-a610-b9f42cd43cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁taught', '▁me', '▁token', 'ization', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ce383-2804-4e9a-a2e7-4edffc44a9a1",
   "metadata": {},
   "source": [
    "Here's what's happening with each token:\n",
    "- '▁IBM': The \"▁\" (often referred to as \"whitespace character\") before \"IBM\" indicates that this token is preceded by a space in the original text. \"IBM\" is kept as is because it's recognized as a whole token by XLNet and it preserves the casing because you are using the \"xlnet-base-cased\" model.\n",
    "- '▁taught', '▁me', '▁token': Similarly, these tokens are prefixed with \"▁\" to indicate they are new words preceded by a space in the original text, preserving the word as a whole and maintaining the original casing.\n",
    "- 'ization': Unlike \"BertTokenizer,\" \"XLNetTokenizer\" does not use \"##\" to indicate subword tokens. \"ization\" appears as its own token without a prefix because it directly follows the preceding word \"token\" without a space in the original text.\n",
    "- '.': The period is tokenized as a separate token since punctuation is treated separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c6ee9-439f-49fd-947d-fc141720db72",
   "metadata": {},
   "source": [
    "## Tokenization with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310dbe0-fb30-42aa-82d1-93d2a0d0732c",
   "metadata": {},
   "source": [
    "In PyTorch, especially with the torchtext library, the tokenizer breaks down text from a data set into individual words or subwords, facilating their converstion into numerical part. After tokenization, the vocab maps these tokens to unique integers, allowing them to be fed into neural networks. This process is vital because deep learning models operate on numerical data and can't process raw text directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e27761-39c8-44fb-bc3b-e9d57372f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abbe3ac-a076-4af7-b5a7-9b06c38198fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3833962d-dec0-4770-a4f7-b3451895aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa990bf-bf0b-4357-ac4b-c4a623b20b90",
   "metadata": {},
   "source": [
    "You apply the tokenizer to the dataset. Note: if basic_english is selected, it returns the basic_english_normalize() function, which normalizes the string first, and then splits it by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d4f311-f7c8-4e4a-ac86-06aa1319dff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cdc39-123a-445b-bf5b-b0c81ffecdbd",
   "metadata": {},
   "source": [
    "## Token Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef6f36-3275-40a9-8437-1dab2ac00ef5",
   "metadata": {},
   "source": [
    "`build_vocab_from_iterator`, the output is typically referred to as `token indices` or simply `indices`. These indices represent the numeric representations of the tokens in the vocabulary. These indices represent the numeric presentations of the tokens in the vocabulary\n",
    "\n",
    "The **```build_vocab_from_iterator```** function, when applied to a list of tokens, assigns a unique index to each token based on its position in the vocabulary. These indices serve as a way to represent the tokens in a numerical format that can be easily processed by machine learning models.\n",
    "\n",
    "For example, given a vocabulary with tokens [\"apple\", \"banana\", \"orange\"], the corresponding indices might be [0, 1, 2], where \"apple\" is represented by index 0, \"banana\" by index 1, and \"orange\" by index 2.\n",
    "\n",
    "`Dataset` is an iterable. Therefore, you use a generator function yield_tokens to apply the `tokenizer`. The purpose of the generator function `yield_tokens` is to yield tokenized texts `one at a time`. Instead of preocessing the entire dataset and returning all the tokenized texts in one go, the generator function processes and yields each tokenized text individually as it is requested. The tokenization process is performed lazily, which means the next tokenized text is generated only when needed, saving memory and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc44a37-c721-42cf-abf3-0f4ad9aef1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7af087-e0a6-46b2-981f-01b01b1d169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0caa8c-85a9-4c25-a04c-d67e0dcb9504",
   "metadata": {},
   "source": [
    "This creates an iterator called **```my_iterator```** using the generator. To begin the evaluation of the generator and retrieve the values, you can iterate over **```my_iterator```** using a for loop or retrieve values from it using the **```next()```** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d9bab8-865c-40bc-88f4-55105b462e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "502ab9c8-8d72-4ff2-adf8-c22b75368174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c2f6ab-2a2f-49d2-8465-3505a409ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basics', 'of', 'pytorch']\n",
      "['nlp', 'techniques', 'for', 'text', 'classification']\n",
      "['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "['sentiment', 'analysis', 'using', 'pytorch']\n",
      "['machine', 'translation', 'with', 'pytorch']\n",
      "['nlp', 'named', 'entity', ',', 'sentiment', 'analysis', ',', 'machine', 'translation']\n",
      "['machine', 'translation', 'with', 'nlp']\n",
      "['named', 'entity', 'vs', 'sentiment', 'analysis', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "for tokens in my_iterator:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20189e7-0b7c-4060-b365-d94395a2cd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df9c7c-5d02-45ef-85e9-cdcf1fb7ffdc",
   "metadata": {},
   "source": [
    "We build a `vocabulary` from the tokenized texts generated by the yield_tokens generator function, which represents the dataset. The build_vocab_from_iterator() function constructs the vocabulary, including a special token `unk` to represent out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a5ece-9330-40e7-b48a-b2650144b518",
   "metadata": {},
   "source": [
    "## Out-of-Vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83cf0c-97c5-46f0-b2f3-f7c24ba8ae3d",
   "metadata": {},
   "source": [
    "When text data is tokenized, there may be words that are not present in the vocabulary because they are rare or unseen during the vocabulary building process. When encountering such OOV words during actual language processing tasks like text generation or language modeling, the model can use the <unk> token to represent them\n",
    "\n",
    "\n",
    "For example, if the word \"apple\" is present in the vocabulary, but \"pineapple\" is not, \"apple\" will be used normally in the text, but \"pineapple\" (being an OOV word) would be replaced by the ```<unk>``` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaa6a484-3cb9-44c3-a93a-3b2c5875a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_new_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1459f53a-666f-4cbe-a7dd-725e0e7da6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a_new_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbdc23d4-5c22-4a84-858a-d862cc4b5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset),specials = ['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# converts each tokens in the dataset as indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fc801-4d1c-4ce7-a031-fa6c489a3db8",
   "metadata": {},
   "source": [
    "This code demonstrates how to fetch a tokenized sentence from an iterator, convert it tokens into indices using a provided vocabulary, and then print both the original sentence and its corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4456ea-35dc-41c4-b6b3-480df6ba27f8",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "1. Got the dataset\n",
    "2. use torchtext.data.utils import get_tokenizer\n",
    "3. Declare a variable --> tokenizer = get_tokenizer('basic_english') --> 'basic_english'  is the model name\n",
    "4. token = tokenizer(dataset[idx][idx]) --> As the dataset is comprised of tuples.\n",
    "5. creates a function which evantually iterates over the dataset and yield the tokenized form\n",
    "\n",
    "    def yield_token(data_iter):\n",
    "        for _,text in data_iter:\n",
    "            yield tokenizer(text)\n",
    "6. To create the vocabulart pass the item through the \n",
    "    build_vocab_from_iterator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec64b81b-3473-4907-9678-7011704d1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:  ['basics', 'of', 'pytorch']\n",
      "Token Indices:  [11, 15, 2]\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_sentences_and_indices(data_iter):\n",
    "    tokenized_sentence = next(data_iter) # Grab single example\n",
    "    token_indices = [vocab[tone] for tone in tokenized_sentence]\n",
    "    return tokenized_sentence,token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentences_and_indices(a_new_iterator)\n",
    "next(a_new_iterator)\n",
    "\n",
    "print(\"Tokenized Sentence: \",tokenized_sentence)\n",
    "print(\"Token Indices: \",token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b007dffe-108f-45df-b89d-0f9e42e04afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['tinon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69660d-d39e-495b-b032-0481529dc6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
