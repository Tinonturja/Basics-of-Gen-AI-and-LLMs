{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2b375d-1771-4c8a-aac5-0906bb817530",
   "metadata": {},
   "source": [
    "Helps prepare and Load Data --> Implemented Using DataLoader class in PyTorch\n",
    "\n",
    "Efficient Batching and Suffling of data\n",
    "\n",
    "Efficient Loading and Preprocessing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f3882-bcff-471f-a9d2-e8dbcd256008",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ad66d-8266-4be7-a13e-b5048ecc933b",
   "metadata": {},
   "source": [
    "1. Get the dataset\n",
    "2. Tokenize the data\n",
    "3. Numerlizing the data\n",
    "4. Fix a constant Batch size\n",
    "5. Turn it into a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216fe70-53e3-4299-a497-cad0ea4b5abf",
   "metadata": {},
   "source": [
    "if we set:\n",
    "    batch_first = `True`\n",
    "    The first dimension in the output tensor will be `batch size`\n",
    "    And the second dimension will be `Sequence Size.`\n",
    "else:\n",
    "    Vice Versa Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0858337-8529-4d77-bbf2-485308f5c0f5",
   "metadata": {},
   "source": [
    "## Installing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183df6de-6f06-4196-8f0a-128153a23f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.42.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (4.42.1)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.30.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from transformers==4.42.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.42.1) (2025.1.31)\n",
      "Requirement already satisfied: sentencepiece in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: spacy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Obtaining dependency information for numpy>=1.19.0 from https://files.pythonhosted.org/packages/2b/3e/e7247c1d4f15086bb106c8d43c925b0b2ea20270224f5186fa48d4fb5cbd/numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Using cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.2.4 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.2.4 which is incompatible.\n",
      "mediapipe 0.10.15 requires numpy<2, but you have numpy 2.2.4 which is incompatible.\n",
      "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.2.4 which is incompatible.\n",
      "scipy 1.11.1 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n",
      "Collecting numpy==1.26.0\n",
      "  Obtaining dependency information for numpy==1.26.0 from https://files.pythonhosted.org/packages/35/21/9e150d654da358beb29fe216f339dc17f2b2ac13fff2a89669401a910550/numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (99 kB)\n",
      "Using cached numpy-1.26.0-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.0 which is incompatible.\n",
      "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchtext==0.17.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (0.17.2)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch==2.2.2) (2025.3.2)\n",
      "Requirement already satisfied: tqdm in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (2.32.3)\n",
      "Requirement already satisfied: numpy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchtext==0.17.2) (1.26.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchdata==0.7.1) (2.3.0)\n",
      "Requirement already satisfied: requests in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchdata==0.7.1) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torchdata==0.7.1) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata==0.7.1) (2025.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchdata==0.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchdata==0.7.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from requests->torchdata==0.7.1) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: portalocker in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: pandas in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tinonturjamajumder/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!pip install numpy==1.26.0\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install torch==2.2.2 torchtext==0.17.2\n",
    "!pip install torchdata==0.7.1\n",
    "!pip install portalocker\n",
    "!pip install numpy pandas\n",
    "!pip install numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02467e17-04e4-4766-9819-834ee2efcd4d",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743cac81-be8b-407d-b151-b535c8e57cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torchtext\n",
    "from transformers import BertTokenizer,XLNetTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchtext.datasets import multi30k,Multi30k\n",
    "from typing import Iterable,List\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5571a3a-a16d-4962-995a-b053f9c4ee70",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5f656-1c3f-4481-9122-2b549542fb3e",
   "metadata": {},
   "source": [
    "## **Data set**\n",
    "\n",
    "A data set in **PyTorch** is an object that represents a collection of data samples. Each data sample typically consists of one or more input features and their corresponding target labels. You can also use your data set to transform your data as needed.\n",
    "\n",
    "## **Data loader**\n",
    "\n",
    "A data loader in **PyTorch** is responsible for efficiently loading and batching data from a data set. It abstracts away the process of iterating over a data set, shuffling, and dividing it into batches for training. In NLP applications, the data loader is used to process and transform your text data, rather than just the data set.\n",
    "\n",
    "Data loaders have several key parameters, including the data set to load from, batch size (determining how many samples per batch), shuffle (whether to shuffle the data for each epoch), and more. Data loaders also provide an iterator interface, making it easy to iterate over batches of data during training.\n",
    "\n",
    "Now, you may ask, '**What is an iterator?**'\n",
    "\n",
    "An iterator is an object that can be looped over. It contains elements that can be iterated through and typically includes two methods, `__iter__()` and `__next__()`. When there are no more elements to iterate over, it raises a **`StopIteration`** exception.\n",
    "\n",
    "Iterators are commonly used to traverse large data sets without loading all elements into memory simultaneously, making the process more memory-efficient. In PyTorch, not all data sets are iterators, but all data loaders are.\n",
    "\n",
    "In PyTorch, the data loader processes data in batches, loading and processing one batch at a time into memory efficiently. The batch size, which you specify when creating the data loader, determines how many samples are processed together in each batch. The data loader's purpose is to convert input data and labels into batches of tensors with the same shape for deep learning models to interpret.\n",
    "\n",
    "Finally, a data loader can be used for tasks such as tokenizing, sequencing, converting your samples to the same size, and transforming your data into tensors that your model can understand.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50dee1-9429-45cb-a0fb-bb07e063c973",
   "metadata": {},
   "source": [
    "## Custom Data set and data loader in PyTorch\n",
    "\n",
    "Defining a CustomDataset which inherits from the torch.utils.data.Dataset class and is initialized with a list of sentences.\n",
    "\n",
    "The Dataset comprises of two essential methods:\n",
    "* *init*(self,sentence): Initializes the data set with a list of sentences.\n",
    "* *getitem*(self,idx): Retrives an item (in this case, a sentence) at a specific index, idx\n",
    "\n",
    "\n",
    "Now, by creating an instance of your custom data set(custom_dataset) by passing in the list of the sentences. Additionally, you can specify a batch_size (batch_size), which determinates how many sentences will be grouped together in each batch during data loading. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff0e16e-29a6-4eac-aed7-9ed40b0bcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"]\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "# Creation of a Custom Dataset\n",
    "class CustomDataset(Dataset): # Dataset is the parent class\n",
    "    def __init__(self,sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self,idx): # retirves an item, get a specific item\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "# Define Batch Size\n",
    "batch_size = 2 # Indicated two sentences are grouped together at a time\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset=custom_dataset,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac62e8f-454d-4e64-9128-ff9f5559489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:[\"Fame's a fickle friend, Harry.\", \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\"]\n",
      "\n",
      "Batch:['Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.', 'Soon we must all face the choice between what is right and what is easy.']\n",
      "\n",
      "Batch:['You are awesome!', 'It is our choices, Harry, that show what we truly are, far more than our abilities.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the Dataloader\n",
    "for batch in dataloader:\n",
    "    print(f\"Batch:{batch}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaaa82d-78c6-438e-84b8-0d56dfb1ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are awesome!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180c4ba-261b-4609-b048-df1081fe1cf0",
   "metadata": {},
   "source": [
    "## Creating Tensor for custom data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bcde6-344d-448c-8130-8023371a3074",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "1. Create a custom dataset\n",
    "2. Pass the dataset through the dataloader, and create batches of input data\n",
    "3. tokenize the input data\n",
    "4. Get the index of the data and ready for to pass thorugh the neural model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0373de0a-6c08-4010-9bba-9cfb8de24758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 sentences:list,\n",
    "                 tokenizer:torchtext.data.utils,\n",
    "                 vocab:torchtext.vocab):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self): #returns the total number of sample in the dataset\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "\n",
    "        # now turn the tokens into indices\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,sentences))\n",
    "\n",
    "# The full process\n",
    "custom_dataset_2 = CustomDataset(sentences=sentences,\n",
    "                                tokenizer=tokenizer,\n",
    "                                vocab=vocab)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfcf2b91-99b1-4ec3-ad56-55f62edd3faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length: 6\n",
      "Sample Items:\n",
      "Item1:tensor([11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "        43, 61,  9, 44,  0, 14,  9, 33,  1])\n",
      "Item2:tensor([35,  6, 16,  3, 38, 40,  0,  8,  1])\n",
      "Item3:tensor([12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "        21,  1])\n",
      "Item4:tensor([54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1])\n",
      "Item5:tensor([66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "         2, 12, 64, 17, 26, 65,  1])\n",
      "Item6:tensor([19,  4, 25, 20])\n"
     ]
    }
   ],
   "source": [
    "# print function\n",
    "print(f\"Custom Dataset Length: {len(custom_dataset_2)}\")\n",
    "print(\"Sample Items:\")\n",
    "\n",
    "for i in range(len(custom_dataset_2)):\n",
    "    sample_item = custom_dataset_2[i]\n",
    "    print(f\"Item{i+1}:{sample_item}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f0811-3377-4eb8-841f-c69e245d4b48",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "005120d7-a124-4c65-83ff-bde111f8310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_2 = CustomDataset(sentences=sentences,\n",
    "                                tokenizer=tokenizer,\n",
    "                                vocab=vocab)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# Create an instance of DataLoader\n",
    "batched_data = DataLoader(dataset=custom_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78e6291a-b553-43b3-8435-190b77f5244d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [9] at entry 0 and [20] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batched_data:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [9] at entry 0 and [20] at entry 1"
     ]
    }
   ],
   "source": [
    "for batch in batched_data:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588b7a7-473c-4559-826e-e8003bcf0c94",
   "metadata": {},
   "source": [
    "Got the error because tensors are of different sizes.\n",
    "\n",
    "If we want to stack tensors, it must be of same sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b07fe-551e-433a-b08f-361e6c4e5bc3",
   "metadata": {},
   "source": [
    "You will encounter an error when attempting to create batches for the tensors. This error arises because the tensor batches have unequal lengths. The data loader is using the default `collate_function`, which requires tensors to have equal lengths. You can define your own `collate_function` and pass the data into it to establish your own rules. Typically, to address the issue of unequal tensor lengths, you employ data padding. This will be demonstrated in the following section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1381-f4d0-4335-8fc8-c84adf240479",
   "metadata": {},
   "source": [
    "## Custom Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f437a-b6a1-4f1e-b70b-415b8570a765",
   "metadata": {},
   "source": [
    "A collate funtion is employed in the context of data loading and batching in ML, particularly when dealing with variable-length data,such as sequences(e.g, text, time series, sequences of events). Its priamry purpose is to prepare and format individual data samples(exm) into batches that can be efficiently processed by machine learning models.\n",
    "\n",
    "* Use custom `custom collate function`\n",
    "* `pad_sequence`: This function is a part of PyTorch and is utilized to pad sequences in a batch, ensuring uniform length. Takes a `batch of sequences` as `input` and pads them to match the length of the longest sequence. The `padding_value=0`argument specifies the value to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ae84656-da3f-43d3-8b3d-03e2f64eee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom collate function\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # Padding the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(batch,batch_first=True,padding_value=0)\n",
    "\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec81ef9-e89f-4e9a-b414-6b69b400bcfa",
   "metadata": {},
   "source": [
    "In the above cell batch_first=`True`; so the shape of the output will be `[batch_size*seq_len]`. If that is false it will be `seq_len*batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8a07643-05cc-41d0-bad0-05fb9f64df27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1,  0,  0,  0,  0,  0],\n",
      "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1]])\n",
      "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1]])\n",
      "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Create a dataloader with this custom collate_fn\n",
    "data_loader_custom_collate = DataLoader(dataset=custom_data_2,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       collate_fn=collate_fn)\n",
    "\n",
    "for batch in data_loader_custom_collate:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a33878fe-7e80-45d0-8c0a-2f5dac9b0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
      "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
      "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader_custom_collate:\n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe35f77-7d3b-4a8f-b7ea-43126bcc6d10",
   "metadata": {},
   "source": [
    "## Full Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437fdc3-e6a0-4b10-b627-e2b7227dd863",
   "metadata": {},
   "source": [
    "### Create Custom Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a800ec9c-94c3-4f8d-ab6c-32f3ec1f033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class CustomDataset(Dataset): # Parent Class Dataset\n",
    "    def __init__(self,\n",
    "                 sentence,\n",
    "                tokenizer,\n",
    "                vocab):\n",
    "        self.sentence = sentence\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self): # get to know the length of the dataset\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __getitem__(self,idx): # grab each item of the custom dataset\n",
    "        tokens = self.tokenizer(self.sentence[idx]) # Grab each sample and convert it into token\n",
    "\n",
    "        # now convert into vocab indices\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "312848f4-7e86-40b4-b7d7-c583dcfaa06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset:\n",
    "corpus = [\n",
    "    \"Ceci est une phrase.\",\n",
    "    \"C'est un autre exemple de phrase.\",\n",
    "    \"Voici une troisième phrase.\",\n",
    "    \"Il fait beau aujourd'hui.\",\n",
    "    \"J'aime beaucoup la cuisine française.\",\n",
    "    \"Quel est ton plat préféré ?\",\n",
    "    \"Je t'adore.\",\n",
    "    \"Bon appétit !\",\n",
    "    \"Je suis en train d'apprendre le français.\",\n",
    "    \"Nous devons partir tôt demain matin.\",\n",
    "    \"Je suis heureux.\",\n",
    "    \"Le film était vraiment captivant !\",\n",
    "    \"Je suis là.\",\n",
    "    \"Je ne sais pas.\",\n",
    "    \"Je suis fatigué après une longue journée de travail.\",\n",
    "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "    \"Je vais chez le médecin cet après-midi.\",\n",
    "    \"La musique adoucit les mœurs.\",\n",
    "    \"Je dois acheter du pain et du lait.\",\n",
    "    \"Il y a beaucoup de monde dans cette ville.\",\n",
    "    \"Merci beaucoup !\",\n",
    "    \"Au revoir !\",\n",
    "    \"Je suis ravi de vous rencontrer enfin !\",\n",
    "    \"Les vacances sont toujours trop courtes.\",\n",
    "    \"Je suis en retard.\",\n",
    "    \"Félicitations pour ton nouveau travail !\",\n",
    "    \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
    "    \"À quelle heure est le prochain train ?\",\n",
    "    \"Bonjour !\",\n",
    "    \"C'est génial !\"\n",
    "]\n",
    "\n",
    "\n",
    "# Set the tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# set the vocab to convert the token into indices\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(iterator = map(tokenizer,corpus))\n",
    "\n",
    "# Create an instance of CustomDataset\n",
    "custom_data = CustomDataset(sentence=corpus,\n",
    "                           tokenizer=tokenizer,\n",
    "                           vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38103dee-2c1e-4b8a-8cc8-957b90518908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 30\n"
     ]
    }
   ],
   "source": [
    "# Show the outputs\n",
    "print(f\"Length of the dataset: {len(custom_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d00ee849-8792-498d-91ff-4a289bdb2e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43,  5, 12, 11,  0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the vocab\n",
    "custom_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "876e0d53-cf39-4d84-a5c8-29898de47826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43,  5, 12, 11,  0])\n",
      "tensor([ 13,   4,   5, 106,  38,  59,   7,  11,   0])\n",
      "tensor([111,  12, 102,  11,   0])\n",
      "tensor([16, 60, 39, 37,  4, 69,  0])\n",
      "tensor([70,  4, 30,  9, 10, 48, 64,  0])\n",
      "tensor([91,  5, 21, 86, 89,  8])\n",
      "tensor([  1, 100,   4,  28,   0])\n",
      "tensor([40, 32,  2])\n",
      "tensor([ 1,  3, 15, 22, 49,  4, 31,  6, 63,  0])\n",
      "tensor([ 81,  53,  84, 105,  51,  75,   0])\n",
      "tensor([ 1,  3, 68,  0])\n",
      "tensor([  6,  62, 116, 113,  42,   2])\n",
      "tensor([ 1,  3, 74,  0])\n",
      "tensor([ 1, 18, 98, 19,  0])\n",
      "tensor([ 1,  3, 61, 33, 12, 73, 71,  7, 23,  0])\n",
      "tensor([ 57,  90, 104,  35,  52,  88,  20,   6, 114,   8])\n",
      "tensor([  1, 108,  46,   6,  79,  44,  34,   0])\n",
      "tensor([10, 78, 29, 17, 80,  0])\n",
      "tensor([ 1, 54, 27, 14, 83, 58, 14, 72,  0])\n",
      "tensor([ 16, 115,  26,   9,   7,  77,  50,  45, 110,   0])\n",
      "tensor([76,  9,  2])\n",
      "tensor([36, 96,  2])\n",
      "tensor([  1,   3,  93,   7, 112,  94,  56,   2])\n",
      "tensor([ 17, 107,  99, 101, 103,  47,   0])\n",
      "tensor([ 1,  3, 15, 95,  0])\n",
      "tensor([65, 20, 21, 82, 23,  2])\n",
      "tensor([  1,   3,  55,  25,   1,  18,  85,  19, 109,  24,  10,  97,   0])\n",
      "tensor([24, 92, 67,  5,  6, 87, 22,  8])\n",
      "tensor([41,  2])\n",
      "tensor([13,  4,  5, 66,  2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(custom_data)):\n",
    "    print(custom_data.__getitem__(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc90fd55-c63b-499e-bcb3-4dec10ef0999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'je',\n",
       " '!',\n",
       " 'suis',\n",
       " \"'\",\n",
       " 'est',\n",
       " 'le',\n",
       " 'de',\n",
       " '?',\n",
       " 'beaucoup',\n",
       " 'la',\n",
       " 'phrase',\n",
       " 'une',\n",
       " 'c',\n",
       " 'du',\n",
       " 'en',\n",
       " 'il',\n",
       " 'les',\n",
       " 'ne',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'ton',\n",
       " 'train',\n",
       " 'travail',\n",
       " 'à',\n",
       " ',',\n",
       " 'a',\n",
       " 'acheter',\n",
       " 'adore',\n",
       " 'adoucit',\n",
       " 'aime',\n",
       " 'apprendre',\n",
       " 'appétit',\n",
       " 'après',\n",
       " 'après-midi',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aujourd',\n",
       " 'autre',\n",
       " 'beau',\n",
       " 'bon',\n",
       " 'bonjour',\n",
       " 'captivant',\n",
       " 'ceci',\n",
       " 'cet',\n",
       " 'cette',\n",
       " 'chez',\n",
       " 'courtes',\n",
       " 'cuisine',\n",
       " 'd',\n",
       " 'dans',\n",
       " 'demain',\n",
       " 'des',\n",
       " 'devons',\n",
       " 'dois',\n",
       " 'désolé',\n",
       " 'enfin',\n",
       " 'est-ce',\n",
       " 'et',\n",
       " 'exemple',\n",
       " 'fait',\n",
       " 'fatigué',\n",
       " 'film',\n",
       " 'français',\n",
       " 'française',\n",
       " 'félicitations',\n",
       " 'génial',\n",
       " 'heure',\n",
       " 'heureux',\n",
       " 'hui',\n",
       " 'j',\n",
       " 'journée',\n",
       " 'lait',\n",
       " 'longue',\n",
       " 'là',\n",
       " 'matin',\n",
       " 'merci',\n",
       " 'monde',\n",
       " 'musique',\n",
       " 'médecin',\n",
       " 'mœurs',\n",
       " 'nous',\n",
       " 'nouveau',\n",
       " 'pain',\n",
       " 'partir',\n",
       " 'peux',\n",
       " 'plat',\n",
       " 'prochain',\n",
       " 'projets',\n",
       " 'préféré',\n",
       " 'que',\n",
       " 'quel',\n",
       " 'quelle',\n",
       " 'ravi',\n",
       " 'rencontrer',\n",
       " 'retard',\n",
       " 'revoir',\n",
       " 'réunion',\n",
       " 'sais',\n",
       " 'sont',\n",
       " 't',\n",
       " 'toujours',\n",
       " 'troisième',\n",
       " 'trop',\n",
       " 'tu',\n",
       " 'tôt',\n",
       " 'un',\n",
       " 'vacances',\n",
       " 'vais',\n",
       " 'venir',\n",
       " 'ville',\n",
       " 'voici',\n",
       " 'vous',\n",
       " 'vraiment',\n",
       " 'week-end',\n",
       " 'y',\n",
       " 'était']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_data.vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d4a3a9f-9ef2-4d78-bb03-69959e54253b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'week-end': 114,\n",
       " 'vraiment': 113,\n",
       " 'voici': 111,\n",
       " 'y': 115,\n",
       " 'ville': 110,\n",
       " 'venir': 109,\n",
       " 'vacances': 107,\n",
       " 'trop': 103,\n",
       " 'troisième': 102,\n",
       " 'toujours': 101,\n",
       " 'sais': 98,\n",
       " 'réunion': 97,\n",
       " 'j': 70,\n",
       " 'revoir': 96,\n",
       " 'retard': 95,\n",
       " \"'\": 4,\n",
       " 'rencontrer': 94,\n",
       " 'que': 90,\n",
       " 'projets': 88,\n",
       " 'cuisine': 48,\n",
       " 'nous': 81,\n",
       " 'plat': 86,\n",
       " 'peux': 85,\n",
       " 'autre': 38,\n",
       " 'pain': 83,\n",
       " 'ceci': 43,\n",
       " 'mœurs': 80,\n",
       " 'médecin': 79,\n",
       " 'là': 74,\n",
       " 'longue': 73,\n",
       " 'de': 7,\n",
       " 'heure': 67,\n",
       " 'une': 12,\n",
       " 'félicitations': 65,\n",
       " 'nouveau': 82,\n",
       " 'français': 63,\n",
       " 'fatigué': 61,\n",
       " 'exemple': 59,\n",
       " 'merci': 76,\n",
       " 'enfin': 56,\n",
       " 'désolé': 55,\n",
       " 'dois': 54,\n",
       " 'prochain': 87,\n",
       " 'française': 64,\n",
       " 'adoucit': 29,\n",
       " 'dans': 50,\n",
       " 'courtes': 47,\n",
       " 'quelle': 92,\n",
       " 'chez': 46,\n",
       " 'pour': 20,\n",
       " 'des': 52,\n",
       " 'ton': 21,\n",
       " 'cette': 45,\n",
       " 'suis': 3,\n",
       " 'cet': 44,\n",
       " 'bon': 40,\n",
       " 'beau': 39,\n",
       " 'matin': 75,\n",
       " 'au': 36,\n",
       " 'après': 33,\n",
       " 'vais': 108,\n",
       " 'journée': 71,\n",
       " 'après-midi': 34,\n",
       " 'apprendre': 31,\n",
       " 'adore': 28,\n",
       " 'acheter': 27,\n",
       " 'musique': 78,\n",
       " 'ne': 18,\n",
       " 'vous': 112,\n",
       " 'un': 106,\n",
       " 'tôt': 105,\n",
       " 'appétit': 32,\n",
       " 'a': 26,\n",
       " 'partir': 84,\n",
       " 'demain': 51,\n",
       " 'devons': 53,\n",
       " 'd': 49,\n",
       " ',': 25,\n",
       " 'as': 35,\n",
       " 'travail': 23,\n",
       " 'lait': 72,\n",
       " 'tu': 104,\n",
       " 'je': 1,\n",
       " 'captivant': 42,\n",
       " 'c': 13,\n",
       " 'à': 24,\n",
       " 'heureux': 68,\n",
       " 'aujourd': 37,\n",
       " 'et': 58,\n",
       " 'est-ce': 57,\n",
       " 't': 100,\n",
       " 'pas': 19,\n",
       " 'beaucoup': 9,\n",
       " '!': 2,\n",
       " 'en': 15,\n",
       " 'du': 14,\n",
       " 'fait': 60,\n",
       " 'la': 10,\n",
       " 'il': 16,\n",
       " 'sont': 99,\n",
       " '?': 8,\n",
       " 'phrase': 11,\n",
       " 'ravi': 93,\n",
       " 'quel': 91,\n",
       " 'était': 116,\n",
       " 'génial': 66,\n",
       " 'train': 22,\n",
       " 'bonjour': 41,\n",
       " 'monde': 77,\n",
       " 'le': 6,\n",
       " 'préféré': 89,\n",
       " 'aime': 30,\n",
       " 'les': 17,\n",
       " '.': 0,\n",
       " 'est': 5,\n",
       " 'hui': 69,\n",
       " 'film': 62}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_data.vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0e398-4d24-42ec-99cd-46e52a01db59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
